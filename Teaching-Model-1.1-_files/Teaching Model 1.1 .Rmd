---
title: "Teaching Model 1.1"
author: "Module2"
date: '2023-02-14'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.



Description: In this project, we will consider the difference with respect to categorical variables such as gender, student major, project, etc. across columns.The goal of the project is to provide inference for statistics and visualization when multiple column variables are available from the practice as well as generating a best model for predicting student performance. The technologies will be used including statistics inference, machine learning PCA and cluster techniques, visualization, factor analysis, bootstramp, etc.are found in this project. In the future projects, we will further provide analysis with time series technology, as well as machine learning other techniques for model prediction regarding the teaching topics.




```{r}

set.seed(1)
setwd("C:/Users/Jing Xie/Documents/R/Teaching Project/Proj 1/Data")
colgadm = read.csv("StudentsAcademicPerformance.csv") 
colgadm = colgadm[0:14,]
colgadm[is.na(colgadm)] = 0
colgadm = colgadm[-13,]
#View(colgadm)

```



Part I


Let's first start from statistical inference for 'Student Academic Performance' Data. There are assumptions from students about gender. For example, they believe gender is important for them to be successful in either career or academia competitions. To help them verify if there is a bias in their opinions, we need to create a model, a teaching model, to guide them to grow. i.e. regarding gender assumption, we need to look into two samples extracted from the raw data with normal distribution assumed. 

```{r}
library(dplyr)
M <- filter(colgadm, Gender == '0')
F<- filter(colgadm, Gender == '1')
a1<- colgadm$Goal
a2 <- colgadm$quiz1
a3<-as.numeric(colgadm$Mid.Test)
a4<-colgadm$HW
a5<-colgadm$Mid.Period
a6<-as.numeric(colgadm$Final.Score)
a7<-as.numeric(colgadm$Project)
a8<-colgadm$Participation
m1 <- M %>% select(Goal, quiz1, Mid.Test, HW, Mid.Period, Final.Score, Project, Participation)
f1 <- F %>% select(Goal, quiz1, Mid.Test, HW, Mid.Period, Final.Score, Project, Participation)
```


```{r}
library(dplyr)
total <- colgadm %>% select(Goal, quiz1, Mid.Test, HW, Mid.Period, Final.Score, Project, Participation)

```

Both methods about p values are not uniformly distributed. We derive the columns are not independent, and must find their correlations. From the graph below, female students are found to have the max variance at around 40 while male students at around 30; and the entire variance is tended to be at 30.

```{r}

library(matrixStats)

cr1 <- as.matrix(m1)
cr2 <- as.matrix(f1)

cr1<- apply(cr1, 2, function(x) as.numeric(x))
cr2<- apply(cr2, 2, function(x) as.numeric(x))

cmsd=rowSds(cr1)       
cfsd=rowSds(cr2)
```



```{r}
library(matrixStats)
total1 <- as.matrix(total)
total1<- apply(total1, 2, function(x) as.numeric(x))
totalsd <- rowSds(total1)
```


```{r}
library(rafalib)
mypar()
shist(cmsd,unit=5,col='blue',xlim=c(15,45), ylim=c(0,5))
shist(cfsd,unit=5.1,col='red',add=TRUE)
shist(totalsd,unit=5,col='black', add=TRUE)
legend("topright",c("Male","Female",'Total'), col=1:2,lty=c(1,1))
```


```{r}
library(rafalib)
mypar()
shist(totalsd,unit=5,col=1,xlim=c(15,45), ylim=c(0,5))
#shist(cfsd,unit=5.1,col=2,add=TRUE)
legend("topright","Total", col=c(1,5),lty=c(1,1))  
  
```


As ascribed, we see the difference of variance of students, but not the variables themselves.

```{r}
total1 <- as.matrix(total)
total1<- apply(total1, 2, function(x) as.numeric(x))
totalsds <- rowSds(t(total1))
mypar()
shist(totalsds,unit=5,col=1,xlim=c(0,25), ylim=c(0,3))
#shist(cfsd,unit=5.1,col=2,add=TRUE)
legend("topright","TotalCol", col=c(1,5),lty=c(1,1))  

```


Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```{r}
ttotalsds=rowSds(t(total1))
#tfsds=rowSds(t(r2))
library(rafalib)
mypar()
shist(ttotalsds,unit=2,col=1,xlim=c(0,20))
#shist(tfsds,unit=0.1,col=2,add=TRUE)
legend("topright",c("TotalColumn"), col=c(1,2),lty=c(1,1))
```

The above are about the variance among rows and columns. From the first plot, male shows the variety of deviation for each row, which indicates the difference of individuals regarding gender. 

The second and the third plots are the column deviation. From the plot, we see the same level in frequency of deviation, which might indicate the indifference among those columns, and it will be verified below.


```{r}
library(tidyverse)
library(ggplot2)
library(cowplot)
```


```{r}
bs <- filter(colgadm, Major =='1')
lw<- filter(colgadm, Major =='2')
md <- filter(colgadm, Major =='3')
cp <- filter(colgadm, Major =='4')
#four <- filter(evaldata, major =='4')
cmb<-bind_rows(list( "1"=bs, "2"=cp, "3"=lw, '4'=md) , .id="Major")

#sex_ev<- filter(comb, sex==1)

```



```{r}
ggplot(cmb, aes(as.numeric(Mid.Test), colours=Major))+
  geom_freqpoly()
ggplot(cmb, aes(as.numeric(Mid.Test), colours=Major, y=..density..))+
    geom_freqpoly()
ggplot(cmb, aes(as.numeric(Participation, Mid.Test), color=Major))+
    geom_density(kernel="gaussian")
ggplot(cmb, aes(as.numeric(Project, Mid.Test), color=Major))+
    geom_density(kernel="gaussian")
```


The above plot indicates the Mid test score impact on these majors. From the plot, mid test has relative more numers below average, but major 




```{r}
ggplot(cmb, aes(as.numeric(Project, Final.Score), color=Major))+
    geom_density(kernel="gaussian")+ggtitle("Plot of Student Final") +
  xlab("Student Project") + ylab("Density")
ggsave('project_final.pdf')
```


This right above plot indicates the project reaction to final score. From the plot, we see cp major, numered 4, has the trend going arise, and md(3) take on two statuses, starting from going down when with a lower project score, and go arise, where the md arrives at a higher density with higher project score. It seems bs(1) major has lower score on project, and its corresponding final density is lower as well. 

```{r}
ggplot(cmb, aes(as.numeric(Participation, Final.Score), color=Major))+
    geom_density(kernel="gaussian")+ggtitle("Plot of Student Final") +
  xlab("Student Participation") + ylab("Density")
ggsave('parti_final.pdf')
```


As for participation component, both md,cp, bs take on the go-arise trend over time, but the lw doesn't find the positive participation relationship with final score. There is one possibility, which is lw students don't take serious manner about their participation. There are two bumps for cp, which indicates two polars who take the serious atttitude and obtain higher score, and who don't take but still achiev a relative higher score. 

```{r}
library(dplyr)
part = colgadm[0:14,7:14]
tcolgadm <- part[-1] %>% t() %>% as.data.frame() %>% setNames(part[,1])
ggplot(stack(part), aes(x = ind, y = values)) +
  labs(x="Samples", y="Frequency") +
  geom_boxplot() 



```


```{r}
ggplot(cmb)+
  geom_boxplot(
    mapping=aes( 
      x = ind,
      y = values
    )
  )+
  coord_flip()+
  xlab("")+
  ylab("Final Test Evaluation, by Major")+
  theme_minimal()


```



```{r}
library(dplyr)
b<- cmb[-1] %>% t() %>% as.data.frame() %>% setNames(cmb[,1])
ggplot(data=stack(b), mapping=aes(x=cmb$Project, y=cmb$Mid.Test))+geom_boxplot()

```

```{r}
# after compared each value across columns, p value from q3 and q6 are significant. It indicates the null hypo about sex at this two are rejected.
gd <- colgadm$Gender
pj <- colgadm$Project
which(colnames(colgadm) == "Mid.Test")   
which(colnames(colgadm) == "Final.Score")   

```


```{r}
cm_mid <- as.numeric(colgadm[,8])
cm_final <- as.numeric(colgadm[,14])
t.test(cm_mid[gd==1],cm_mid[gd==0])$p.value
t.test(cm_final[gd==1],cm_final[gd==0])$p.value
```



```{r}
library(matrixStats)
a<- colgadm$Mid.Curved
b<- colgadm$Mid.Test
c<-colgadm$HW
d<-colgadm$Participation
e<- colgadm$Mid.Period
f<-colgadm$Final.Raw
g<-colgadm$Final.Score
h<-colgadm$Project
e <- colgadm$quiz1
A <- model.matrix(~b+c-1)
cat("ncol=",ncol(A),"rank=", qr(A)$rank,"\n")

```
Eventually, we have removed the confound variables (5.6) for those categorical columns. The following is to create a model that is used to find the predict of y. This is the first model we have so far. Now we need to calculate the coeff.  and its AIC  ### 5.7


```{r}
alpha <- 0.05
m = 10000
N = 10
dta <- 3
nullHypothesis <- c( rep(TRUE,900), rep(FALSE,100))
set.seed(1)
calls <- sapply(1:m, function(i){
  control <- sample(cladm,N)
  treatment <- sample(cladm,N)
  if(!nullHypothesis[i]) treatment <- treatment + dta
  ifelse( t.test(treatment,control)$p.value < alpha, 
          "Called Significant",
          "Not Called Significant")
})


```

```{r}
null_hypothesis <- factor( nullHypothesis, levels=c("TRUE","FALSE"))



```



```{r}
b <- 10 
VandS <- replicate(B,{
  calls <- sapply(1:m, function(i){
    control <- sample(cladm,N)
    treatment <- sample(cladm,N)
    if(!nullHypothesis[i]) treatment <- treatment + delta
    t.test(treatment,control)$p.val < alpha
  })
  cat("V =",sum(nullHypothesis & calls), "S =",sum(!nullHypothesis & calls),"\n")
  c(sum(nullHypothesis & calls),sum(!nullHypothesis & calls))
  })


```
After checking the collinearity, we are able to construct a model as above, called Y



We also can use the following approach to build up information to create a model. First, we need to provide description to the dataset.



```{r}
fg <- colgadm%>%select('Gender', 'Final.Score')
fp <- colgadm%>%select('Project', 'Final.Score')
mp <- colgadm%>%select('Project', 'Mid.Test')

```


```{r}
fg <- data.matrix(fg)  # Using data.matrix for numeric column dataframe
set.seed(1)
N = 10
B = 100000
fgpvals <- replicate(B,{
  mal = sample(fg,N)
  fmal = sample(fg,N)
  t.test(mal,fmal)$p.val 
  })
hist(fgpvals, main = 'Student Gender and Final Performance', xlab = 'pvals')

```



```{r}
fp <- data.matrix(fp)
set.seed(1)
N = 10
B = 100000
fppvals <- replicate(B,{
  pj0 = sample(fp,N)
  pj1 = sample(fp,N)
  t.test(pj0,pj1)$p.val
  })
hist(fppvals, main = 'Student Final Performance and Project', xlab = 'Student Project')

```



```{r}
mp <- data.matrix(mp)
set.seed(1)
N = 10
B = 100000
mppvals <- replicate(B,{
  pj0 = sample(mp,N)
  pj1 = sample(mp,N)
  t.test(pj0,pj1)$p.val 
  })
hist(mppvals)

```
Inference 1: All above histogram plots indicate the variable such as gender and project provide the uniform like shape about student academic performance in mid and final, where, in this test, we use t test, after a verification to its distribution.  




```{r}
cladm <- colgadm[,8:14]
cladm <- apply(as.matrix(cladm), 2, as.numeric)
library(rafalib)
mypar(1,2)
qqnorm(cladm[gd==0])
qqline(cladm[gd==0])
qqnorm(cladm[gd==1])
qqline(cladm[gd==1])


```


After verifying distribution in term of gender, we corrected an incorrect assumption about gender.
```{r}
cladmttest <- function(x) t.test(x[gd==1],x[gd==0],var.equal=TRUE)$p.value
cladmpvals1 <- apply(cladm,1,cladmttest)
cladmpvals2 <- apply(cladm,2,cladmttest)

cladmpvals1
cladmpvals2
sum(cladmpvals1<0.05)

```


```{r}
cladmttestPJ <- function(x) t.test(x[pj==1],x[pj==0],var.equal=TRUE)$p.value
cladmpvals3 <- apply(cladm,1,cladmttestPJ)
cladmpvals4 <- apply(cladm,2,cladmttestPJ)

cladmpvals3
cladmpvals4
sum(cladmpvals3<0.05)

```

The above p value for students and for column variables don't show the significance level regarding the gender, however, when considering project, we find it has an impact on some column variables such as the test during the mid test (Multiple Test Section)


```{r}
set.seed(1)
library(genefilter)
u <- nrow(colgadm)
v <- ncol(colgadm[,8:14])
randomcladm <- matrix(rnorm(u*v),u,v)
cladmttest <- function(x) t.test(x[gd==1],x[gd==0],var.equal=TRUE)$p.value
cladmnulpval2 <- apply(randomcladm,2,cladmttest)
cladmnulpval1 <- apply(randomcladm,1,cladmttest)
cladmnulpval2
cladmnulpval1
which(cladmnulpval1 < 0.1)   
which(cladmnulpval2 < 0.1)

```


```{r}
#nullpvals <- colttests(randomData,h)$p.value
plot(cladmnulpval1,-log10(cladmnulpval1),
     xlab="Effect size",ylab="- log (base 10) p-values")
```


```{r}
#nullpvals <- colttests(randomData,h)$p.value
plot(cladmnulpval2,-log10(cladmnulpval2),
     xlab="Effect size",ylab="- log (base 10) p-values")
```  


The plots above indicate the effect size of either samples or column variables. From the plots, we see there are at least 3 variables in columns have bigger effect size. From gender perspective, mid test and final test are found to be impacted. (multiple test section)




```{r}
cladmttestPJ <- function(x) t.test(x[pj==1],x[pj==0],var.equal=TRUE)$p.value
cladmnulpval5 <- apply(randomcladm,1,cladmttestPJ)
cladmnulpval6 <- apply(randomcladm,2,cladmttestPJ)

cladmnulpval5
cladmnulpval6
which(cladmnulpval5 < 0.1)   
which(cladmnulpval6 < 0.1)

```

```{r}
plot(cladmpvals4,-log10(cladmpvals4),               # for particular sample
     xlab="Effect size",ylab="- log (base 10) p-values")
plot(cladmnulpval6,-log10(cladmnulpval6),
     xlab="Effect size",ylab="- log (base 10) p-values")  # for randomness

```


The above tests are the indication of model analysis regarding the project, where we have 90 percent of confidence to reveal at least 4 column variables take important roles in the model, in which randomness isn't as good as specified case; however, we expect the more outcomes with the confidence level, so we will further consider other methods than the t test (multiple test section)


```{r}
#nullpvals <- colttests(randomData,h)$p.value
plot(cladmnulpval5,-log10(cladmnulpval5),
     xlab="Effect size",ylab="- log (base 10) p-values")
``` 


```{r}

#nullpvals <- colttests(randomData,h)$p.value
plot(cladmnulpval6,-log10(cladmnulpval6),
     xlab="Effect size",ylab="- log (base 10) p-values")
``` 

